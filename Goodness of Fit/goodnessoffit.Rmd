---
output: pdf_document
---

```{r echo=FALSE, message=FALSE, warning=FALSE}
bernoulli_mle <- function(input_data)
{
  bernoulli_mle <- mean(input_data)
  return(bernoulli_mle)
}


binomial_mle <-function(input_data,n)
{
  m <- length(input_data)
  binomial_mle <- mean(input_data)/n
  return(binomial_mle)
}

poisson_mle <- function(input_data)
{
  lambda_value_mle <- mean(input_data)
  return(lambda_value_mle)
}

geometric_mle <- function(input_data)
{
  geometric_mle <- (1.0/(mean(input_data)+1))
  return(geometric_mle)
}

exponential_mle <- function(input_data)
{
  beta_mle <- 1/mean(input_data)
  return(beta_mle)
}

normal_mle <-function(input_data)
{
  n <- length(input_data)
  mean_mle <- mean(input_data)
  variance_mle <- sum((input_data-mean_mle)^2)/n
  return(c(mean_mle,variance_mle))
}

uniform_mle <-function(input_data)
{
  a_mle <- min(input_data)
  b_mle <- max(input_data)
  return(c(a_mle,b_mle))
}

gamma_mle <-function(input_data)
{
  s <- log(mean(input_data)) - mean(log(input_data))
  alpha_mle <- (3-s)/(12*s) + sqrt((s-3)^2 + 24*s)/(12*s)
  beta_mle <- mean(input_data)/alpha_mle
  return(c(alpha_mle,beta_mle))
}

beta_mle <- function(input_data)
{
  # Initializing alpha and beta parameters using Method of moment estimates
  input_data_mean <- mean(input_data)
  input_data_variance <- (sum(input_data * input_data))/length(input_data)
  a_mle <- ((input_data_mean ^ 2) - (input_data_mean * input_data_variance))/(input_data_variance - (input_data_mean ^ 2))
  b_mle <- (a_mle * (1 - input_data_mean))/(input_data_mean)
  final_val <- c(a_mle, b_mle)
  
  for(index in 1:100){
    
    g1 <- digamma(a_mle) - digamma(a_mle + b_mle) - (sum(log(input_data)))/length(input_data)
    
    g2 <- digamma(b_mle) - digamma(a_mle + b_mle) - (sum(log(1 - input_data))/length(input_data))
    
    g <- c(g1, g2)
    
    # Calculating G matrix of second derivatives:
    G1_val <- trigamma(a_mle) - trigamma(a_mle + b_mle)
    G2_val <- -trigamma(a_mle + b_mle)
    G3_val <- trigamma(b_mle) - trigamma(a_mle + b_mle)
    G <- matrix(c(G1_val, G2_val, G2_val, G3_val), nrow = 2, ncol = 2, byrow = TRUE)
    G_inverse <- solve(G)
    
    # Final values for the iteration: Theta_mle(i+1) = Theta_mle(i) - G_inverse*g
    
    final_val <- final_val - t(G_inverse %*% g)
    a_mle <- final_val[1]
    b_mle <- final_val[2]
  }
  
  return(c(a_mle,b_mle))
  
}


chi_square_mle <- function(input_data)
{
  ## Here we have used the property of chi-square being a special case of Gamma distribution.
  ## A gamma distribution (alpha,beta) with alpha = v/2 and beta = 1/2, is a chi-squared RV with v degrees of freedom.
  result_mle <- gamma_mle(input_data)
  v_mle <- 2* result_mle[1]
  return(v_mle)
}

library(MASS)
multinomial_mle <- function(input_data){
  a = nrow(input_data)
  p = c(0,0,0,0,0)
  for(i in 1:a)
    p[i]<-1-((var(input_data[i,]))/mean(input_data[i,]))
  n = sum(rowMeans(input_data))/sum(p[1:a])
  return (n)
}

multivariate_mle <- function(input_data){
  mu_hat = colMeans(input_data)
  summation = var(input_data)
}

library(Rlab)
```


# Goodness of Fit #

Prepared by: Josh Levine (jl2108), Harsh Patel (hkp49), Jaini Patel (jp1891), Yifan Liao (yl1463) and Aayush Shah (avs93). 

### Assignment ###

Using the ks.test - Build a goodness of fit test for each distribution using the parametric bootstrap based on maximum likelihood for every instance in the MOM and MLE estimator program (not including multivariate normal).

### What is Goodness of Fit? ###

The goodness of fit test is a statistical hypothesis test to see how well sample data fit a distribution from a population with a normal distribution.

Goodness-of-fit tests are statistical tests aiming to determine whether a set of observed values match those expected under the applicable model.   
There are multiple types of goodness-of-fit tests, but the most common is the chi-square test.  
These tests can show you whether your sample data fit an expected set of data from a population with normal distribution.      

## ##

### Understanding Goodness Of Fit ###

A small p-value (typically <= 0.05) indicates strong evidence against the null hypothesis, so you reject the null hypothesis.  

A large p-value (> 0.05) indicates weak evidence against the null hypothesis, so you fail to reject the null hypothesis.  

p-values very close to the cutoff (0.05) are considered to be marginal (could go either way). Always report the p-value so your readers can draw their own conclusions.  

\pagebreak

## Build a goodness of fit test for each distribution using the parametric bootstrap based on maximum likelihood ##

We will be using the same MLE functions from our previous report. Included below is the function that runs the goodness of fit test for a given distribution. Comments have been written for the first distribution as the other calculations are similar. 

```{r}
# Parametric Bootstrap using KS Test
goodness_of_fit.func <- function(distribution, nboot = 1000, input_data)
{
  print(paste("DISTRIBUTION:", distribution, sep=" " ))
  mle_name = get(paste(distribution, "_mle",sep = ""))
  n <- length(input_data)

  #binomail calculates theta_hat slightly differently then the rest
  if(distribution == "binomial")
  {
    theta_hat = mle_name(input_data, n)
  }
  else
  {
    theta_hat = mle_name(input_data)
  }
  
  if(distribution == "geometric")
  {
    q_hat <- qgeom(c(1:n)/(n+1),theta_hat)#calculate q_hat using this distributions q function
    D0 <- ks.test(input_data, q_hat)$statistic#calculate D0 using ks test
    Dvec<-NULL
    
    for(i in 1:nboot){
      x_star <- rgeom(n, theta_hat)#generate a random distribution
      theta_hat_star <- mle_name(x_star)#use the mle function previously generated
      
      q_hat_star <- qgeom(c(1:n)/(n+1), theta_hat_star)#calculate q hat star
      D_star <- ks.test(x_star, q_hat_star)$statistic#use ks test on the data
      Dvec <- c(Dvec, D_star)#combine the vectors for each iteration of the loop
    }
    hist(Dvec)#plot the d vector
    p_value <- sum(Dvec > D0)/nboot #calculate and return the p value
    return(list("p_value" = p_value))
  }
  else if(distribution == "bernoulli")
  {
    Dvec<-NULL
    q_hat <- qbern(c(1:n)/(n+1),theta_hat)
    D0 <- ks.test(input_data, q_hat)$statistic#calculate D0 using ks test
    for(i in 1:nboot){
    vecb <- rbinom(1000, 10, 0.5)
    est.val = mle_name(vecb)
    est.val[2] = est.val[1]
    est.val[1] = 1
    Dvec <- c(Dvec, c(ks.test(vecb, est.val[1], est.val[2])$statistic))
    }
    hist(Dvec)
    p_value <- sum(Dvec > D0)/nboot #calculate and return the p value
    return(list("p_value" = p_value))
  }
  else if(distribution == "binomial")
  {
    q_hat <- qbinom(c(1:n)/(n+1), n,theta_hat)
    
    D0 <- ks.test(input_data, q_hat)$statistic
    Dvec<-NULL
    
    for(i in 1:nboot){
      x_star <- rbinom(1000, 10, 0.5)
      theta_hat_star <- mle_name(x_star, n)
      
      q_hat_star <- qbinom(c(1:n)/(n+1), n, theta_hat_star)
      D_star <- ks.test(x_star, q_hat_star)$statistic
      Dvec <- c(Dvec, D_star)
    }
    hist(Dvec)
    p_value <- sum(Dvec > D0)/nboot
    return(list("p_value" = p_value))
  }
  else if(distribution == "poisson"){
    q_hat <- qpois(c(1:n)/(n+1),theta_hat)
    
    D0 <- ks.test(input_data, q_hat)$statistic
    Dvec<-NULL
    
    for(i in 1:nboot){
      x_star <- rpois(n, theta_hat)
      theta_hat_star <- mle_name(x_star)
      
      q_hat_star <- qpois(c(1:n)/(n+1), theta_hat_star)
      D_star <- ks.test(x_star, q_hat_star)$statistic
      Dvec <- c(Dvec, D_star)
    }
    hist(Dvec)
    p_value <- sum(Dvec > D0)/nboot
    return(list("p_value" = p_value))
  }
  else if(distribution == "normal"){
    q_hat <- qnorm(c(1:n)/(n+1),mean = theta_hat[1], sd = theta_hat[2])
    
    D0 <- ks.test(input_data, q_hat)$statistic
    Dvec<-NULL
    
    for(i in 1:nboot){
      x_star <- rnorm(n,mean = theta_hat[1], sd =theta_hat[2])
      theta_hat_star <- mle_name(x_star)
      
      q_hat_star <- qnorm(c(1:n)/(n+1),mean = theta_hat_star[1], sd =theta_hat_star[2])
      D_star <- ks.test(x_star, q_hat_star)$statistic
      Dvec <- c(Dvec, D_star)
    }
    hist(Dvec)
    p_value <- sum(Dvec > D0)/nboot
    return(list("p_value" = p_value))
  }
  else if(distribution == "uniform"){
    q_hat <- qunif(c(1:n)/(n+1), theta_hat[1], theta_hat[2])
    
    D0 <- ks.test(input_data, q_hat)$statistic
    Dvec<-NULL
    
    for(i in 1:nboot){
      x_star <- runif(n, theta_hat[1], theta_hat[2])
      theta_hat_star <- mle_name(x_star)
      
      q_hat_star <- qunif(c(1:n)/(n+1), theta_hat_star[1], theta_hat_star[2])
      D_star <- ks.test(x_star, q_hat_star)$statistic
      Dvec <- c(Dvec, D_star)
    }
    hist(Dvec)
    p_value <- sum(Dvec > D0)/nboot
    return(list("p_value" = p_value))
  }
  else if(distribution == "gamma"){
    q_hat <- qgamma(c(1:n)/(n+1), shape = theta_hat[1],  scale = theta_hat[2])
    
    D0 <- ks.test(input_data, q_hat)$statistic
    Dvec<-NULL
    
    for(i in 1:nboot){
      x_star <- rgamma(n, shape = theta_hat[1], scale = theta_hat[2])
      theta_hat_star <- mle_name(x_star)
      
      q_hat_star <- qgamma(c(1:n)/(n+1), shape = theta_hat_star[1], scale = theta_hat_star[2])
      D_star <- ks.test(x_star, q_hat_star)$statistic
      Dvec <- c(Dvec, D_star)
    }
    hist(Dvec)
    p_value <- sum(Dvec > D0)/nboot
    return(list("p_value" = p_value))
  }
  else if(distribution == "beta"){
    q_hat <- qbeta(c(1:n)/(n+1),shape1 = abs(theta_hat[1]), shape2 = abs(theta_hat[2]))
    D0 <- ks.test(input_data, q_hat)$statistic
    Dvec<-NULL
    
    for(i in 1:nboot){
      x_star <- rbeta(n, shape1 =  theta_hat[1],shape2 =  theta_hat[2])
      theta_hat_star <- mle_name(x_star)
      
      q_hat_star <- qbeta(c(1:n)/(n+1), shape1 =  abs(theta_hat_star[1]), shape2 = abs(theta_hat_star[2]))
      
      D_star <- ks.test(x_star, q_hat_star)$statistic
      Dvec <- c(Dvec, D_star)
    }
    hist(Dvec)
    p_value <- sum(Dvec > D0)/nboot
    return(list("p_value" = p_value))
  }
  else if(distribution == "exponential"){
    q_hat <- qexp(c(1:n)/(n+1),theta_hat)
    
    D0 <- ks.test(input_data, q_hat)$statistic
    Dvec<-NULL
    
    for(i in 1:nboot){
      x_star <- rexp(n, theta_hat)
      theta_hat_star <- mle_name(x_star)
      
      q_hat_star <- qexp(c(1:n)/(n+1), theta_hat_star)
      D_star <- ks.test(x_star, q_hat_star)$statistic
      Dvec <- c(Dvec, D_star)
    }
    hist(Dvec)
    p_value <- sum(Dvec > D0)/nboot
    return(list("p_value" = p_value))
  }
  else if(distribution == "chi_square"){
    q_hat <- qchisq(c(1:n)/(n+1),theta_hat)
    D0 <- ks.test(input_data, q_hat)$statistic
    Dvec<-NULL
    
    for(i in 1:nboot){
      x_star <- rchisq(n, theta_hat)
      theta_hat_star <- mle_name(x_star)
      
      q_hat_star <- qchisq(c(1:n)/(n+1), theta_hat_star)
      D_star <- ks.test(x_star, q_hat_star)$statistic
      Dvec <- c(Dvec, D_star)
    }
    hist(Dvec)
    p_value <- sum(Dvec > D0)/nboot
    return(list("p_value" = p_value))
  }
  else if(distribution == "multinomial"){
   q_hat <- qchisq(c(1:n)/(n+1),theta_hat)
    D0 <- ks.test(input_data, q_hat)$statistic
    Dvec<-NULL
    p = c(0.15,0.05,0.4,0.1,0.3)
    for(i in 1:nboot){
      x_star <- rmultinom(10000,size=5,p)
      theta_hat_star <- mle_name(x_star)
      
      q_hat_star <- qchisq(c(1:n)/(n+1), theta_hat_star)
      D_star <- ks.test(x_star, q_hat_star)$statistic
      Dvec <- c(Dvec, D_star)
    }
    hist(Dvec)
    p_value <- sum(Dvec > D0)/nboot
    return(list("p_value" = p_value))
  }
}
```

\pagebreak

Below are the outputs (p-values) of the Goodness of Fit Tests for each of the distributions along with a graph of the d-values before the p-value is calculated.

```{r warning=FALSE}
goodness_of_fit.func("bernoulli", input_data = rbern(1000, 0.5))
```

\pagebreak

```{r warning=FALSE}
goodness_of_fit.func("geometric", input_data = rgeom(1000, 0.5))
```

\pagebreak

```{r warning=FALSE}
goodness_of_fit.func("binomial", input_data = rbinom(1000, 10, 0.5))
```

\pagebreak

```{r warning=FALSE}
goodness_of_fit.func("poisson", input_data = rpois(1000, 0.5))
```

\pagebreak

```{r warning=FALSE}
goodness_of_fit.func("normal", input_data = rnorm(1000, 0, 1))
```

\pagebreak

```{r warning=FALSE}
goodness_of_fit.func("uniform", input_data = runif(1000, 0, 100))
```

\pagebreak

```{r warning=FALSE}
goodness_of_fit.func("gamma", input_data = rgamma(1000, shape = 5, scale = 20))
```

\pagebreak

```{r warning=FALSE}
goodness_of_fit.func("beta", input_data = rbeta(1000, shape1 = 4.2, shape2 = 2.7))
```

\pagebreak

```{r warning=FALSE}
goodness_of_fit.func("exponential", input_data = rexp(1000, 2))
```

\pagebreak

```{r warning=FALSE}
goodness_of_fit.func("chi_square", input_data = rchisq(1000, 2))
```

\pagebreak

```{r warning=FALSE}
p = c(0.15,0.05,0.4,0.1,0.3)
input_data = rmultinom(10000,size=5,p)
goodness_of_fit.func("multinomial", input_data = input_data)
```

\pagebreak

## Results: ##

Below is the table that shows the p-value of each distribution.

# #
# #
# #
# #
# #
# #
# #

## Conclusion: ##
 
