---
output:
  pdf_document: 
    fig_width: 10
    fig_height: 8
  html_document: default
---
```{r echo=FALSE, warning=FALSE, message=FALSE}
library(prophet)
library(quantmod)
library(forecast)
library("xlsx")
library(tseries)
library(timeSeries)
library(dplyr)
library(fGarch)

#daily value data
dji.daily<-read.csv("C:\\Users\\Josh\\Documents\\Grad School\\Statistics\\Final Project\\dji_d.csv")
dji.daily.ts <-ts(dji.daily$Open, start=1896, end=2020, frequency=253)

#weekly value data
dji.weekly<-read.csv("C:\\Users\\Josh\\Documents\\Grad School\\Statistics\\Final Project\\dji_WEEKLY.csv")
dji.weekly.ts <-ts(dji.weekly$Average, start=1896, end=2020, frequency=52)

#Get the past 5 years to use as our data
dji.daily.ts <- window(dji.daily.ts, start=2000, end=2020)
dji.weekly.ts <- window(dji.weekly.ts, start=2000, end=2020)
```

# Dow Jones Index Estimator #

Prepared by: Josh Levine (jl2108), Harsh Patel (hkp49), Jaini Patel (jp1891), Yifan Liao (yl1463) and Aayush Shah (avs93). 

## Goal ##

Our goal is to analyze the moving average index of the Dow Jones using a several different models including ARIMA, GARCH, Feed Forward Neural Network, KNN and . In our analysis we aim to predict one day or one week in the future of the index. And then analyze our prediction. 

## The Data ##

We will be using two data sets, one is index data on a daily basis and the other on a weekly basis. The data starts in 1896 and runs into 2020. Our starting file was the daily index and we wrote a python script to combine in by week. In our analysis we are using the opening data as that days index value.   

&nbsp;
  
<center> First few rows of the DAILY index CSV file used </center>
|Date      |Open    |High    |Low     |Close   |Volume    |
|----------|--------|--------|--------|--------|----------|
|1896-05-27|29.39   |29.39   |29.39   |29.39   |          |
|1896-05-28|29.11   |29.11   |29.11   |29.11   |          |
|1896-05-29|29.43   |29.43   |29.43   |29.43   |          |
|1896-06-01|29.4    |29.4    |29.4    |29.4    |          |
|1896-06-02|29      |29      |29      |29      |          |
|1896-06-03|28.8    |28.8    |28.8    |28.8    |          |
|1896-06-04|28.93   |28.93   |28.93   |28.93   |          |
|1896-06-05|29.2    |29.2    |29.2    |29.2    |          |
|1896-06-08|28.83   |28.83   |28.83   |28.83   |          |  

&nbsp;

<center> First few rows of the WEEKLY index CSV file we used </center>
|Year|Week Number|Average           |
|----|-----------|------------------|
|1896|22         |29.33             |
|1896|23         |29.065999999999995|
|1896|24         |28.429999999999996|
|1896|25         |29.074            |
|1896|26         |27.666000000000004|
|1896|27         |25.479999999999997|
|1896|28         |25.310000000000002|
|1896|29         |23.796            |
|1896|30         |22.742            |

\pagebreak

## Python script used to generate the weekly CSV using the daily CSV ##
```{r eval=FALSE}
import csv #import needed to read and write csv files
import datetime #import to handle date time objects

values = [] #initialize the values array

#open the csv file
with open("C:\\Users\\Josh\\Documents\\Grad School\\Statistics\\Final Project\\dji_d.csv") as csvfile:
    reader = csv.reader(csvfile, delimiter=',')#create the reader object
    next(reader)#skip the header
    for row in reader:#iterate over each row to create the values array 
        dateString = row[0]#get the string value of the date (i.e 6/20/1978)
        #Turn the date string into a date time object
        #We do this so we can extract week data from the data
        date = datetime.datetime.strptime(dateString, '%Y-%m-%d').date()
        #Add to the values list the year, the week of the year and the index
        values.append([date.year, date.isocalendar()[1], float(row[1])])
        
weeklyAverage = [] #initialize the weeklyAverage array

                    #Year           Week Number      Index (float)     Count
#Add the first row to weeklyAverages
weeklyAverage.append([values[0][0], values[0][1], float(values[0][2]), 1]) 
#Iterate over each row of the values array to add to the weeklyAverages array
for i in range(len(values)): 
    flag = True #flag if a row in the averages array has been created or not
    print(values[i][0]) #Just making sure the program is running
    for j in range(len(weeklyAverage)): #Iterate over the weeklyAverages array
#If a row in the weekly averages has been created for this year and week,
#add to that row/ If not create a new row for that week of that year
        if(values[i][0] == weeklyAverage[j][0] and values[i][1] == weeklyAverage[j][1]): 
            weeklyAverage[j][2] += float(values[i][2])#Add to the index total of that specific week
            weeklyAverage[j][3] += 1 #Add to the index of that week by 1
            flag = False #set the flag to false because a row is already created for this week
    if flag: #If the flag was never set to false, create a row for that week and set the count to 1
        #starting a row for that week of that year
        weeklyAverage.append([values[i][0], values[i][1], values[i][2], 1]) 

#Now the data has been combined in a weekly manner
#Next is to create the new csvfile for further analysis

#Open a new csv file
file = open("C:\\Users\\Josh\\Documents\\Grad School\\Statistics\\Final Project\\dji_WEEKLY.csv", 'w', newline ='') 
with file:   
    header = ['Year', 'Week Number', 'Average'] #Create the header for our new csv file
    writer = csv.DictWriter(file, fieldnames = header)#Create a writer object 
    writer.writeheader() #Writes the header
    
    for k in range(len(weeklyAverage)): #Iterate over the weekly averages to write to the csv
        #Writes the the three column values to the csv
        #Also calculates averege week by diving total index by count
        writer.writerow({'Year' : weeklyAverage[k][0],  
                         'Week Number': weeklyAverage[k][1],  
                         'Average': weeklyAverage[k][2]/weeklyAverage[k][3]})
        #Another check just to make sure everything is running
        print(weeklyAverage[k][2]/weeklyAverage[k][3]) 
```

The script is fully commented, so you may go through each line to learn how it was combined weekly.

##

## Analysis with ARIMA: Auto Regressive Integrated Moving Average ##

Arima models are used when we can assume the data we are working with is non-stationary. And this is clearly true when it comes to Dow Jones Index data. We say that time series data sets are stationary when their means, variance and auto-covariance don't change during time. Most economic Time Series are non-stationary. We can apply ARIMA models to the index or any stock price of our choosing. We denote the forecasting model by ARIMA(p, d, q) where:

${{Y_t=c+\phi_ly_{dt}-1+\phi_py_{dtp}+...+\theta_1e_{t-1}+\theta_qe_{tq}+e_t}}$ 

In ARIMA, p denotes the number of auto regressive terms, d denotes the number of time that the set should be differentiated for making it stationary and the last parameter q denotes the number of invertible moving average terms. Creating the model is done by iterative approaches using 4 steps.

1. Identification: Find the best values reproducing the time series variable to forecast.
2. Analysis and Differentiation: Study the time series, in our study we will use different statistical tools such as ACF and PACF tests.
3. Adjust the Arima Model: Extract and determine coefficients and adjust the model.
4. Prediction: Once the best model has been selected, we can make a forecast based on probabilistic future values.

For our approach we will be using the auto.arima function that will automatically return the best ARIMA model according to the data.

First we will conduct an ADF test for both our daily and weekly index values:

```{r}
adf.test(dji.daily.ts)
adf.test(dji.weekly.ts)
```

After conducting the ADF test we now apply the ACF (Autocorrelation Function) and PACF (Partial Autocorrelation Function) to both the daily and weekly data set.
```{r}
acf(dji.daily.ts)
```

\pagebreak

```{r}
acf(dji.weekly.ts)
```

\pagebreak

```{r}
pacf(dji.daily.ts)
```

\pagebreak

```{r}
pacf(dji.weekly.ts)
```

Autocorrelation refers to how correlated a time series is with its past data points. As in Autoregressive models, the ACF will decrease exponentially, that can be seen in both the daily and weekly ACF graphed above. The ACF is used to display the plot used to see the correlation between the points, up to and including the lag unit. We observe in both the daily and weekly data sets that the autocorrelations are significant for large number of lags. To identify the (p) order of the autoregressive model we use the PACF plot. For moving average models we use the ACF plot to identify the (q) order. Looking at both the daily and weekly PACF plots, we can see a significant spike ONLY at the first lag, which means that all higher order autocorrelations are effectively explained by the first lag autocorrelation. As we plan to use the auto.arima function which gives us the best approach for our data, we will not dive too deep into the analysis of finding model parameters. 

\pagebreak

### Using the auto.arima function on both the daily and weekly data sets: ###
```{r}
#Run the auto.arima function
daily.arima.model <- auto.arima(dji.daily.ts, lambda = "auto")
weekly.arima.model <- auto.arima(dji.weekly.ts, lambda = "auto")

#display the arima model for both the daily and weekly data
daily.arima.model
weekly.arima.model
```

##
The details of the accuracy function are defined as:

1. ME: Mean Error - average of all the errors in a set.
2. RMSE: Root Mean Squared Error -  standard deviation of the residuals (prediction errors). Residuals are a measure of how far from the regression line data points are.
3. MAE: Mean Absolute Error - the average of all absolute errors.
4. MPE: Mean Percentage Error - the computed average of percentage errors by which forecasts of a model differ from actual values of the quantity being forecast.
5. MAPE: Mean Absolute Percentage Error - a statistical measure of how accurate a forecast system is. It measures this accuracy as a percentage.
6. MASE: Mean Absolute Scaled Error - a scale-free error metric that gives each error as a ratio compared to a baselineâ€™s average error.
7. ACF1: Autocorrelation of errors at lag 1 - the correlation between values that are one time period apart. More generally, a lag k autocorrelation is the correlation between values that are k time periods apart.

\pagebreak

```{r}
#display the accuracy of that model for the daily and weekly data set
accuracy(daily.arima.model)
accuracy(weekly.arima.model)
```

###

Now that we have our model summary we can check the residuals of the model. The RESIDUALS in a time series model are what is left over after fitting a model. The residuals are equal to the difference between the observations and the corresponding fitted values. Residuals are useful in checking whether a model has adequately captured the information in the data. A good forecasting method will yield residuals with the following properties:

1. The residuals are uncorrelated. If there are correlations between residuals, then there is information left in the residuals which should be used in computing forecasts.
2. The residuals have zero mean. If the residuals have a mean other than zero, then the forecasts are biased

Below is the mean and plot of the residuals for both the daily and weekly data sets. We can see that the daily set has a mean very close to zero while the weekly has a much larger mean. This means the data set for daily data will be better at forecasting than the weekly data set.
```{r}
mean(daily.arima.model$residuals)
mean(weekly.arima.model$residuals)
```

\pagebreak

## Visualization of the Daily Residuals ##
```{r}
plot(daily.arima.model$residuals)
```

\pagebreak

```{r}
hist(daily.arima.model$residuals)
```

\pagebreak

## Visualization of the Weekly Residuals ##
```{r}
plot(weekly.arima.model$residuals)
```

\pagebreak

```{r}
hist(weekly.arima.model$residuals)
```

As we can see both data sets do have a lot of residuals that hang around zero but the weekly data set has more outliers than the daily data set. Both do have a mostly normal curve, allowing us to believe that we are good to continue our study. Finally, we will do our last residual plot using the tsdiag function which will give us the Standardized Residuals, ACF of Residuals and the p values for Ljung-Box statistic plots.

\pagebreak

## TSDIAG of the Daily data set ##
```{r}
tsdiag(daily.arima.model)
```

\pagebreak

## TSDIAG of the Weekly data set ##
```{r}
tsdiag(weekly.arima.model)
```

We now focus our analysis of the model to the Ljung-Box p-values. Our null hypothesis is as follows:
1. H0: The data set points are independently distributed.

With this as our null hypothesis, a significant p value greater than 0.5 does not reject the fact that the data points are not correlated. (Data points not correlated because we are using Dow Jones Index data). We can now analyze the lag at which the p value is the smallest. For our daily data set that appears to be Lag = 10. And for our Weekly data set that appears to be Lag = 6

```{r}
Box.test(daily.arima.model$residuals, lag= 10, type="Ljung-Box")
Box.test(weekly.arima.model$residuals, lag= 6, type="Ljung-Box")
```

We observe both p values are rather large, so this does not bode well for our model. We continue anyway because we would like to see this models prediciton.

```{r}
Box.test(daily.arima.model$residuals, type="Ljung-Box")
Box.test(weekly.arima.model$residuals, type="Ljung-Box")
```

In our generalized box test we can see our null hypothesis is still not rejected, so we continue our study with confidence. Displayed below is a graph of our original time series over 5 years for both the daily and weekly data. 

\pagebreak

## Daily Time Series ##
```{r}
plot(dji.daily.ts)
```

\pagebreak

## Weekly Time Series ##
```{r}
plot(dji.weekly.ts)
```

# Auto Arima Results #

Below is the function that gets the forecasts for both Daily and Weekly data sets. We are predicting 30 days in advanced for the daily data set. We are predicting 4 weeks in advanced for the weekly data set. As seen below h represents the number of prediction we are making.
```{r}
daily.forecast <- forecast(daily.arima.model, h=30)
weekly.forecast <- forecast(weekly.arima.model, h=4)
```

\pagebreak

## Daily forecast 30 days out ##
```{r}
plot(daily.forecast, include=250)
```

Note: We are only displaying the prior 250 days because it visualizes better.

\pagebreak

## Weekly forecast 4 weeks out ##
```{r}
plot(weekly.forecast, include=75)
```

Note: We are only displaying the prior 75 weeks because it visualizes better.

We can see in both our daily and weekly forecast that the blue line represents our mean predictions. The daily graph shows a slight increase over 30 days. While our weekly graph shows the index staying more or less the same. Now lets dig into some of the predictive statistics.

```{r}
head(daily.forecast$mean)
head(weekly.forecast$mean)
```

The daily MEAN forecasts shows a slight increase from the start of its predictions! While the weekly shows an exactly same index over time, which certainly is not the case.

##
Upper Case Scenario:
```{r}
head(daily.forecast$upper)
head(weekly.forecast$upper)
```

##
Lower Case Scenario:
```{r}
head(daily.forecast$lower)
head(weekly.forecast$lower)
```

